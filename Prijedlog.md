# Sinteza funkcije nagrade iz opisa u prirodnom jeziku transformerom

## Područje

Duboko Učenje

## Opis teme

Novi smjerovi istraživanja u podržanom učenju izrodili su algoritme koji eliminiraju pretpostavku da funkcija nagrade zadovoljava Markovljevo svojstvo. Takve funkcije nagrade mogu obuhvatiti probleme u kojima je nagrada rijetka i ovisi o nizu događaja u vremenu, no trebaju ih zadati inženjeri u obliku konačnih automata, LTL formula, ili regularnih izraza.

U području obrade prirodnog jezika najveće nedavne pomake donijela je nova arhitektura transformera koja omogućuje da se znanje iz općenite domene modeliranja jezika prenese na uže domene specifičnih zadataka, odnosno da naši modeli dubokog učenja mogu imati koristi od polaznih parametara koji su pronađeni s veoma visokim budžetom u procesorskom vremenu i podacima.

Diplomski rad će omogućiti zadavanje funkcija nagrade za podržano učenje na razgovorni način njihovim opisom u prirodnom jeziku, te će rad s tehnologijom podržanog učenja na domenama koje nisu Markovljeve učiniti bržim i dostupnijim. U tu svrhu razvit će se transdukcijski model na bazi transformera i grafičko sučelje koje za korisnika vizualizira rezultate njegovog upita u prirodnom jeziku kroz agenta koji obavlja taj zadatak.

## Literatura

- Vaswani et. al., *Attention Is All You Need*, (2017.)
- Icarte et. al., *Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning*, (2020.)
- Lewis Tunstall, Leandro von Werra, Thomas Wolf, *Natural Language Processing with Transformers*, O'Reilly (2022.)
- Richard S. Sutton, Andrew G. Barto, *Reinforcement Learning*, A Bradford Book, (2018.)
